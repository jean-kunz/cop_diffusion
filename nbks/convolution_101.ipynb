{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp conv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from icecream import ic\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    train = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    val = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def data_loaders(train_data, val_data, batch_size):\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "training_data, validation_data = load_cifar()\n",
    "#training_data, validation_data = load_fashion_mnist()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size=10\n",
    ")\n",
    "x_train_var = np.var(training_data.data / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(x):\n",
    "    x = make_grid(x.cpu().detach() + 0.5)\n",
    "    x = x.numpy()\n",
    "    ic(x.shape)\n",
    "    ic(np.transpose(x,(1,2,0)).shape)\n",
    "    fig = plt.imshow(np.transpose(x, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, by =next(iter(training_loader))\n",
    "bx = bx.to(device)\n",
    "ic(bx.shape)\n",
    "ic(by)\n",
    "display_image_grid(bx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution basics\n",
    "\n",
    "https://www.telesens.co/2017/09/17/cnn_derivatives/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_tensor, title: str):\n",
    "    # Convert the image tensor to a NumPy array\n",
    "    image_np = image_tensor.squeeze().cpu().detach().numpy()\n",
    "    image_np = np.transpose(image_np, (1, 2, 0))  # Rearrange dimensions to HWC\n",
    "\n",
    "    # normalized tze image, because imshow can take value from -1 .. 1\n",
    "    image_np_norm = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    plt.imshow(image_np_norm)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1\n",
    "kernel_size = 3\n",
    "padding = 2\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "\n",
    "x = bx[0].unsqueeze(0)\n",
    "show_image(x, \"Image (no padding)\")\n",
    "if padding>0:\n",
    "    x = F.pad(x, (padding, padding, padding, padding))\n",
    "    show_image(x, \"After padding\")\n",
    "\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = x.size()\n",
    "#out_channels, _, kernel_height, kernel_width = kernel_weight.size()\n",
    "kernel_height, kernel_width = kernel_size, kernel_size\n",
    "ic(input_height, input_width,kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "# Calculate the dimensions of the output tensor\n",
    "output_height = (input_height - kernel_height) // stride + 1\n",
    "output_width = (input_width - kernel_width) // stride + 1\n",
    "ic(output_height, output_width)\n",
    "\n",
    "# Initialize the output tensor\n",
    "output = torch.zeros(batch_size, out_channels, output_height, output_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_line_kernel = torch.tensor(\n",
    "    [[[[1, 0, -1], [1, 0, -1], [1, 0, -1]]]], dtype=torch.float32\n",
    ").to(device)\n",
    "ic(vertical_line_kernel.shape)\n",
    "vertical_line_kernel = vertical_line_kernel.repeat(3, 3, 1, 1)\n",
    "ic(vertical_line_kernel.shape)\n",
    "ic(vertical_line_kernel.shape)\n",
    "kernel_weight = vertical_line_kernel\n",
    "\n",
    "for i in range(0, input_height - kernel_height + 1, stride):\n",
    "    for j in range(0, input_width - kernel_width + 1, stride):\n",
    "        region = x[:, :, i : i + kernel_height, j : j + kernel_width]\n",
    "        #rk = region * kernel_weight\n",
    "        #output[:, :, i // stride, j // stride] = torch.sum(rk, dim=(1,2,3)) #+ kernel_bias\n",
    "        output[:, :, i // stride, j // stride]=torch.tensordot(region, kernel_weight,dims=([1,2,3], [1, 2, 3]))#+kernel_bias\n",
    "ic(output.shape)\n",
    "show_image(x, 'input')\n",
    "show_image(output, f\"convolution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution logic\n",
    "vertical_line_kernel = torch.tensor(\n",
    "    [[[[1, 0, -1], [1, 0, -1], [1, 0, -1]]]], dtype=torch.float32\n",
    ").to(device)\n",
    "# Replicate the kernel for each input channel (R, G, B) and each output channel.\n",
    "vertical_line_kernel = vertical_line_kernel.repeat(3, 3, 1, 1)\n",
    "ic(vertical_line_kernel.shape)\n",
    "\n",
    "horiz_line_kernel = torch.tensor([[[1, 1, 1], [0, 0, 0], [-1, -1, -1]]], dtype=torch.float32).to(device).repeat(3,3,1,1)\n",
    "\n",
    "# Laplacian filter, which detects edges regardless of their orientation.\n",
    "laplacian_kernel = torch.tensor([[[ 0,  1,  0],\n",
    " [ 1, -4,  1],\n",
    " [ 0,  1,  0]]], dtype=torch.float32).to(device).repeat(3,3,1,1)\n",
    "\n",
    "emboss_kernel = torch.tensor([[[ -2,  -1,  0],\n",
    " [ -1, 1,  1],\n",
    " [ 0,  1,  2]]], dtype=torch.float32).to(device).repeat(3,3,1,1)\n",
    "\n",
    "\n",
    "pic_kernel = torch.tensor([[[ -5,  0,  0],\n",
    " [ -5, 0,  0],\n",
    " [ -5,  0,  0]]], dtype=torch.float32).to(device).repeat(3,3,1,1)\n",
    "\n",
    "kernel_weight = vertical_line_kernel\n",
    "convol_type: str = 'vertical lines'\n",
    "kernel_weight = horiz_line_kernel\n",
    "convol_type: str = 'horiz. lines'\n",
    "kernel_weight = laplacian_kernel\n",
    "convol_type: str = 'laplacian'\n",
    "kernel_weight = emboss_kernel\n",
    "convol_type: str = \"emboss\"\n",
    "kernel_weight = pic_kernel\n",
    "convol_type: str = \"pic\"\n",
    "\n",
    "\n",
    "#kernel_bias = torch.randn(out_channels).to(device)\n",
    "\n",
    "for i in range(0, input_height - kernel_height + 1, stride):\n",
    "    for j in range(0, input_width - kernel_width + 1, stride):\n",
    "        region = x[:, :, i : i + kernel_height, j : j + kernel_width]\n",
    "        #rk = region * kernel_weight\n",
    "        #output[:, :, i // stride, j // stride] = torch.sum(rk, dim=(1,2,3)) #+ kernel_bias\n",
    "        output[:, :, i // stride, j // stride]=torch.tensordot(region, kernel_weight,dims=([1,2,3], [1, 2, 3]))#+kernel_bias\n",
    "\n",
    "show_image(output, f\"{convol_type} convolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results with pytorch implementation\n",
    "# Define a convolution layer with the pre-defined kernel\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, groups=1, bias=False).to(device)\n",
    "conv.weight = nn.Parameter(kernel_weight)\n",
    "\n",
    "ic(conv.weight.shape)\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv(x)\n",
    "\n",
    "show_image(output, f\"Convolved {convol_type} with pytorch implem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized version of conv2d\n",
    "\n",
    "First you must understand unfold operation on tensor to extract sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First unfold op is key to extract windows on the tensor\n",
    "sx = (\n",
    "    torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    ")  # Shape (1, 1, 4, 4)\n",
    "\n",
    "# we want to extract  non overlapping windows of 2x2\n",
    "\n",
    "ic(sx);\n",
    "ic(sx.shape)\n",
    "usx = sx.unfold(dimension=2, size=2, step=2) #Extracts 2x2 blocks along the height dimension (axis 2) with a stride/step of 2.\n",
    "ic(\"after unfold on height\")\n",
    "ic(usx.shape)\n",
    "ic(usx);\n",
    "usx = usx.unfold(dimension=3, size=2, step=2) #Extracts 2x2 blocks along the width dimension (axis 3) with a stride/step of 2.\n",
    "ic(\"after unfold on width\")\n",
    "ic(usx.shape);\n",
    "ic(usx);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results on image\n",
    "ic(x.shape)\n",
    "nb_win = 2\n",
    "win_size = int(x.shape[-1]/nb_win)\n",
    "\n",
    "ux = x.unfold(2,win_size,win_size).unfold(3,win_size,win_size)\n",
    "ic(ux.shape)\n",
    "show_image(x[0],\"Whole image\")\n",
    "\n",
    "def show_image_windows(unfolded_x, max_nb:int=-1):\n",
    "    nb : int = 1\n",
    "    for h in range(0,unfolded_x.shape[2]):\n",
    "        for w in range(0,unfolded_x.shape[3]):\n",
    "            show_image(unfolded_x[0,:,h,w,:,:], f\"{h},{w} window\");\n",
    "            nb += 1\n",
    "            if (max_nb>0) & (nb >= max_nb):\n",
    "                return\n",
    "\n",
    "\n",
    "show_image_windows(ux, max_nb=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, in_channels, in_height, in_width = x.shape\n",
    "out_channels, in_channels, kernel_height, kernel_width = kernel_weight.shape\n",
    "ic(out_channels, in_channels, kernel_height,kernel_width)\n",
    "# Calculate output dimensions\n",
    "out_height = (in_height - kernel_height) // stride + 1\n",
    "out_width = (in_width - kernel_width) // stride + 1\n",
    "\n",
    "# # Extract sliding local blocks\n",
    "input_unfolded = x.unfold(2, kernel_height, stride).unfold(3, kernel_width, stride)\n",
    "#show_image_windows(input_unfolded, max_nb=5)\n",
    "\n",
    "# Flatten sliding blocks\n",
    "flatten_unfolded_input=input_unfolded.contiguous().view(batch_size, in_channels, out_height * out_width, kernel_height * kernel_width)\n",
    "ic(flatten_unfolded_input.shape)\n",
    "show_image(flatten_unfolded_input,\"flatten unfolded\")\n",
    "\n",
    " # Reshape for matrix multiplication\n",
    "flatten_unfolded_input_reshaped = flatten_unfolded_input.permute(0, 2, 1, 3).contiguous().view(batch_size * out_height * out_width, in_channels * kernel_height * kernel_width)\n",
    "ic(flatten_unfolded_input_reshaped)\n",
    "weight_reshaped = kernel_weight.view(out_channels, -1).t()\n",
    "ic(weight_reshaped.shape)\n",
    "\n",
    "# Perform matrix multiplication and add bias\n",
    "flatten_output = torch.matmul(flatten_unfolded_input_reshaped, weight_reshaped)\n",
    "ic(flatten_output.shape)\n",
    "output = flatten_output.view(batch_size, out_height, out_width, out_channels).permute(0, 3, 1, 2)\n",
    "ic(output.shape)\n",
    "show_image(output, \"After vectorized convolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MishaLaskin/vqvae/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_pool2d(input, kernel_size, stride):\n",
    "    batch_size, channels, height, width = input.size()\n",
    "    output_height = (height - kernel_size) // stride + 1\n",
    "    output_width = (width - kernel_size) // stride + 1\n",
    "\n",
    "    output = torch.zeros((batch_size, channels, output_height, output_width), device=input.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c in range(channels):\n",
    "            for i in range(0, height - kernel_size + 1, stride):\n",
    "                for j in range(0, width - kernel_size + 1, stride):\n",
    "                    h_start, h_end = i, i + kernel_size\n",
    "                    w_start, w_end = j, j + kernel_size\n",
    "                    output[b, c, i // stride, j // stride] = torch.max(input[b, c, h_start:h_end, w_start:w_end])\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "\n",
    "max_output = max_pool2d(output, kernel_size=2, stride=2)\n",
    "show_image(max_output,\"MaxPool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pytorch maxpool is the same.\n",
    "max_pool = nn.MaxPool2d(kernel_size=1, stride=4)\n",
    "mean_pool = nn.AvgPool2d(kernel_size=1, stride=4)\n",
    "\n",
    "# Apply the Max Pooling layer to the input tensor\n",
    "max_output = max_pool(output)\n",
    "mean_output = mean_pool(output)\n",
    "show_image(max_output,\"Pytorch MaxPool\")\n",
    "show_image(mean_output,\"Pytorch MeanPool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized vesion\n",
    "\n",
    "1. Unfold : unfold is used to extract sliding local blocks from the input tensor. This creates a tensor where each window of size kernel_size x kernel_size is extracted according to the stride.\n",
    "\n",
    "2. Reshape: The tensor is reshaped to merge the kernel dimensions, preparing it for the max operation.\n",
    "\n",
    "3. Max Operation: The max operation is applied over the last dimension, which contains the elements of each kernel window, to get the maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size=2\n",
    "stride=2\n",
    "unfold = output.unfold(2, kernel_size,stride)\n",
    "ic(output.shape)\n",
    "ic(unfold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_max_pool2d(input, kernel_size, stride):\n",
    "    # Use unfold to extract sliding local blocks from a batched input tensor\n",
    "    unfolded = input.unfold(2, kernel_size, stride).unfold(3, kernel_size, stride)\n",
    "    # Now unfolded tensor has shape: (batch_size, channels, output_height, output_width, kernel_size, kernel_size)\n",
    "    # Reshape to merge the kernel dimensions\n",
    "    unfolded = unfolded.contiguous().view(*unfolded.size()[:4], -1)\n",
    "    # Apply max operation over the last dimension (which contains the kernel elements)\n",
    "    output, _ = unfolded.max(dim=-1)\n",
    "    return output\n",
    "\n",
    "max_output = vectorized_max_pool2d(output, kernel_size=2, stride=2)\n",
    "show_image(max_output,\"Vectorized MaxPool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
