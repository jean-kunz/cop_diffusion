{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 64\n",
    "num_embeddings = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "patch_size = 7\n",
    "\n",
    "# Data Loading\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_x, tr_y = next(iter(train_loader))\n",
    "tr_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) -> torch.Size([1, 1, 4, 28, 7]) 4x7(patch_size) = 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tr_x[:1]\n",
    "u_x = x.unfold(2, patch_size, patch_size)\n",
    "print(x.shape, \"->\", u_x.shape, \"4x7(patch_size) = 28\")\n",
    "u_x.unfold(3, patch_size, patch_size).shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3,  4,  5],\n",
      "          [ 6,  7,  8,  9, 10, 11],\n",
      "          [12, 13, 14, 15, 16, 17],\n",
      "          [18, 19, 20, 21, 22, 23],\n",
      "          [24, 25, 26, 27, 28, 29],\n",
      "          [30, 31, 32, 33, 34, 35]]]])\n",
      "tensor([[[[[ 0,  6, 12],\n",
      "           [ 1,  7, 13],\n",
      "           [ 2,  8, 14],\n",
      "           [ 3,  9, 15],\n",
      "           [ 4, 10, 16],\n",
      "           [ 5, 11, 17]],\n",
      "\n",
      "          [[18, 24, 30],\n",
      "           [19, 25, 31],\n",
      "           [20, 26, 32],\n",
      "           [21, 27, 33],\n",
      "           [22, 28, 34],\n",
      "           [23, 29, 35]]]]])\n",
      "tensor([[[[[[ 0,  1,  2],\n",
      "            [ 6,  7,  8],\n",
      "            [12, 13, 14]],\n",
      "\n",
      "           [[ 3,  4,  5],\n",
      "            [ 9, 10, 11],\n",
      "            [15, 16, 17]]],\n",
      "\n",
      "\n",
      "          [[[18, 19, 20],\n",
      "            [24, 25, 26],\n",
      "            [30, 31, 32]],\n",
      "\n",
      "           [[21, 22, 23],\n",
      "            [27, 28, 29],\n",
      "            [33, 34, 35]]]]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 28x28 tensor with values from 0 to 783\n",
    "t_size = 6\n",
    "t = torch.arange(0, t_size * t_size).view(t_size, t_size).unsqueeze(0).unsqueeze(0)\n",
    "t_bs = 1\n",
    "print(t)\n",
    "t_patch_size = 3\n",
    "# Tensor.unfold(dimension, size, step)\n",
    "# Returns a view of the original tensor which contains all slices of size size from self tensor in the tensor dimension.\n",
    "# use a step (like a stride)\n",
    "ut = t.unfold(2, t_patch_size, t_patch_size)\n",
    "print(ut)\n",
    "ut = ut.unfold(3, t_patch_size, t_patch_size)\n",
    "print(ut)\n",
    "utv = ut.contiguous().view(t_bs, 1, -1, t_patch_size, t_patch_size)\n",
    "bputv = utv.view(-1, 1, t_patch_size, t_patch_size)\n",
    "# it should create 4 examples of patches of 3x3.\n",
    "assert bputv.shape == (\n",
    "    (t_size // t_patch_size) * (t_size // t_patch_size),\n",
    "    1,\n",
    "    t_patch_size,\n",
    "    t_patch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int, patch_size: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * patch_size * patch_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(\n",
    "            3, self.patch_size, self.patch_size\n",
    "        )\n",
    "        x = x.contiguous().view(batch_size, 1, -1, self.patch_size, self.patch_size)\n",
    "        # each patch in each example is added as an example.\n",
    "        x = x.view(-1, 1, self.patch_size, self.patch_size)\n",
    "        # print(\"after patch creation\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * patch_size * patch_size)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 64, patch_size, patch_size)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_patches_per_img: 16.0\n",
      "batch_size: 128\n",
      "sm_x: torch.Size([128, 1, 28, 28])\n",
      "enc_x: torch.Size([2048, 2])\n",
      "dec_x: torch.Size([2048, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# check encode/decode logic works\n",
    "sm_latent_dim = 2\n",
    "img_side_size = 28\n",
    "nb_patches_per_img = (img_side_size / patch_size) ** 2\n",
    "print(\"nb_patches_per_img:\", nb_patches_per_img)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "enc = Encoder(sm_latent_dim, patch_size=patch_size)\n",
    "dec = Decoder(sm_latent_dim)\n",
    "sm_x = tr_x[:batch_size]\n",
    "batch_size = sm_x.size(0)\n",
    "print(\"sm_x:\", sm_x.shape)\n",
    "sm_enc_x = enc(sm_x)\n",
    "print(\"enc_x:\", sm_enc_x.shape)\n",
    "assert sm_enc_x.shape[1] == sm_latent_dim\n",
    "assert sm_enc_x.shape[0] == nb_patches_per_img * batch_size\n",
    "sm_dec_x = dec(sm_enc_x)\n",
    "assert sm_dec_x.shape == (\n",
    "    nb_patches_per_img * batch_size,\n",
    "    1,\n",
    "    patch_size,\n",
    "    patch_size,\n",
    "), \"We should reconstruct each patch based on its latent representation\"\n",
    "print(\"dec_x:\", sm_dec_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer:\n",
    "Encoder maps x -> z_e (latent space)\n",
    "\n",
    "It quantizes the latent vectors to the nearest vector in the codebook, aka transform the encoder output into a discrete one-hot vector that is the index of the closest embedding vector z_e -> z_q\n",
    "\n",
    "Decoder maps z_q -> x_hat to reconstruct the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| embedding.weight.data: tensor([[-0.1857, -0.0251],\n",
      "                                   [ 0.8152,  0.0820],\n",
      "                                   [ 1.8598, -0.7282],\n",
      "                                   [ 0.5367, -0.0859],\n",
      "                                   [ 1.2928,  0.0050]])\n",
      "ic| embedding.weight.data: tensor([[ 0.1782, -0.1790],\n",
      "                                   [-0.0828,  0.0235],\n",
      "                                   [-0.0739, -0.0994],\n",
      "                                   [-0.0048, -0.0440],\n",
      "                                   [-0.1914, -0.0950]])\n",
      "ic| z.shape: torch.Size([2048, 2])\n",
      "ic| z_flattened.shape: torch.Size([2048, 2])\n",
      "ic| distances.shape: torch.Size([2048, 5])\n",
      "ic| encoding_indices.shape: torch.Size([2048, 1])\n",
      "ic| z_flattened[3:6]: tensor([[-0.0297,  0.0819],\n",
      "                              [-0.0297,  0.0819],\n",
      "                              [ 0.0405,  0.0268]], grad_fn=<SliceBackward0>)\n",
      "ic| encoding_indices[3:6]: tensor([[1],\n",
      "                                   [1],\n",
      "                                   [3]])\n",
      "ic| z_q[3:6]: tensor([[-0.0828,  0.0235],\n",
      "                      [-0.0828,  0.0235],\n",
      "                      [-0.0048, -0.0440]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0828,  0.0235],\n",
       "        [-0.0828,  0.0235],\n",
       "        [-0.0048, -0.0440]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_num_embeddings = 5\n",
    "embedding = nn.Embedding(sm_num_embeddings, sm_latent_dim)\n",
    "ic(embedding.weight.data)\n",
    "embedding.weight.data.uniform_(-1 / sm_num_embeddings, 1 / sm_num_embeddings)\n",
    "ic(embedding.weight.data)\n",
    "\n",
    "z = sm_enc_x\n",
    "ic(z.shape)\n",
    "z_flattened = z.view(-1, sm_latent_dim)\n",
    "ic(z_flattened.shape)\n",
    "distances = torch.cdist(z_flattened, embedding.weight)\n",
    "ic(distances.shape)\n",
    "\n",
    "encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "ic(encoding_indices.shape)\n",
    "\n",
    "z_q = embedding(encoding_indices).view(z.shape)\n",
    "ic(z_flattened[3:6])\n",
    "ic(encoding_indices[3:6])\n",
    "ic(z_q[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Quantizer\n",
    "# wrap the logic into a nn.Module\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        dist_type: Literal[\"cosine\", \"euclidean\"] = \"euclidean\",\n",
    "    ):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "        self.dist_type = dist_type\n",
    "\n",
    "    def get_cosine_similarity_indices(self, z):\n",
    "        # Normalize the latent vectors and the embedding vectors to unit length\n",
    "        z_normalized = F.normalize(z, dim=-1)\n",
    "        embedding_normalized = F.normalize(self.embedding.weight, dim=-1)\n",
    "        # Compute cosine similarity\n",
    "        similarity = torch.matmul(z_normalized, embedding_normalized.t())\n",
    "        # Get the indices of the highest similarity (closest vectors)\n",
    "        encoding_indices = torch.argmax(similarity, dim=-1).unsqueeze(1)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_euclid_dist_indices(self, z):\n",
    "        z_flattened = z.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(z_flattened, self.embedding.weight)\n",
    "        # Get the indices with smallest euclidean distance\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        return encoding_indices\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.dist_type == \"cosine\":\n",
    "            encoding_indices = self.get_cosine_similarity_indices(z)\n",
    "        else:\n",
    "            encoding_indices = self.get_euclid_dist_indices(z)\n",
    "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
    "        return z_q, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0017,  0.0019],\n",
       "         [-0.0017,  0.0019],\n",
       "         [-0.0017,  0.0019],\n",
       "         [-0.0017,  0.0019]], grad_fn=<SliceBackward0>),\n",
       " tensor([[20],\n",
       "         [20],\n",
       "         [20],\n",
       "         [20]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq = VectorQuantizer(num_embeddings, sm_latent_dim, dist_type=\"euclidean\")\n",
    "sm_z_q, sm_z_q_idx = vq(sm_enc_x)\n",
    "assert sm_z_q.shape == sm_enc_x.shape\n",
    "sm_z_q[:4], sm_z_q_idx[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, latent_dim: int, num_embeddings: int, patch_size: int):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, patch_size=patch_size)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, _ = self.vector_quantizer(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, z, z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_recon.shape: torch.Size([2048, 1, 7, 7])\n",
      "    z.shape: torch.Size([2048, 2])\n",
      "    z_q.shape: torch.Size([2048, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 1, 7, 7]), torch.Size([2048, 2]), torch.Size([2048, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae = VQVAE(\n",
    "    latent_dim=sm_latent_dim, num_embeddings=sm_num_embeddings, patch_size=patch_size\n",
    ")\n",
    "x_recon, z, z_q = vqvae(sm_x)\n",
    "ic(x_recon.shape, z.shape, z_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/b3p9tbn949714k6ymvrnpslm0000gq/T/ipykernel_92903/4185263665.py:14: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([2048, 1, 7, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = F.mse_loss(x_recon, x)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (28) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Visualization of reconstructed images\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_reconstructed\u001b[39m(model, data_loader):\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m x_recon, z, z_q \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 14\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_recon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m commitment_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(z_q, z\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m commitment_loss\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cop-diffusion-04jlPuwc-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cop-diffusion-04jlPuwc-py3.12/lib/python3.12/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (28) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "model = VQVAE(latent_dim, num_embeddings, patch_size=patch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z, z_q = model(x)\n",
    "            recon_loss = F.mse_loss(x_recon, x)\n",
    "            commitment_loss = F.mse_loss(z_q, z.detach())\n",
    "            loss = recon_loss + commitment_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "train(model, train_loader, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# Visualization of reconstructed images\n",
    "def show_reconstructed(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            x_recon, _, _ = model(x)\n",
    "            break\n",
    "    x = x.cpu().numpy()\n",
    "    x_recon = x_recon.cpu().numpy()\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(x[0][0], cmap=\"gray\")\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[1].imshow(x_recon[0][0], cmap=\"gray\")\n",
    "    axes[1].set_title(\"Reconstructed\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_reconstructed(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
