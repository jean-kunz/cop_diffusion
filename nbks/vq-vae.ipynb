{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from typing import Literal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 64\n",
    "num_embeddings = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "patch_size = 7\n",
    "\n",
    "# Data Loading\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x, tr_y = next(iter(train_loader))\n",
    "tr_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tr_x[:1]\n",
    "u_x = x.unfold(2, patch_size, patch_size)\n",
    "print(x.shape, \"->\",u_x.shape, \"4x7(patch_size) = 28\")\n",
    "u_x.unfold(3, patch_size, patch_size).shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 28x28 tensor with values from 0 to 783\n",
    "t_size=6\n",
    "t = torch.arange(0, t_size*t_size).view(t_size, t_size).unsqueeze(0).unsqueeze(0)\n",
    "t_bs = 1\n",
    "print(t)\n",
    "t_patch_size = 3\n",
    "# Tensor.unfold(dimension, size, step)\n",
    "# Returns a view of the original tensor which contains all slices of size size from self tensor in the tensor dimension.\n",
    "# use a step (like a stride)\n",
    "ut = t.unfold(2,t_patch_size,t_patch_size)\n",
    "print(ut)\n",
    "ut = ut.unfold(3,t_patch_size,t_patch_size)\n",
    "print(ut)\n",
    "utv = ut.contiguous().view(t_bs, 1, -1, t_patch_size, t_patch_size)\n",
    "bputv = utv.view(-1, 1, t_patch_size, t_patch_size)\n",
    "#it should create 4 examples of patches of 3x3.\n",
    "assert bputv.shape  == ((t_size//t_patch_size)*(t_size//t_patch_size),1, t_patch_size, t_patch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim:int, patch_size:int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64*patch_size*patch_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(batch_size, 1, -1, self.patch_size, self.patch_size)\n",
    "        # each patch in each example is added as an example.\n",
    "        x = x.view(-1, 1, self.patch_size, self.patch_size)\n",
    "        #print(\"after patch creation\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64*patch_size*patch_size)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 64, patch_size, patch_size)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check encode/decode logic works\n",
    "sm_latent_dim = 2\n",
    "img_side_size = 28\n",
    "nb_patches_per_img = (img_side_size/patch_size)**2\n",
    "print(\"nb_patches_per_img:\", nb_patches_per_img)\n",
    "print(\"batch_size:\",batch_size)\n",
    "\n",
    "enc = Encoder(sm_latent_dim, patch_size=patch_size)\n",
    "dec = Decoder(sm_latent_dim)\n",
    "sm_x = tr_x[:batch_size]\n",
    "batch_size = sm_x.size(0)\n",
    "print(\"sm_x:\", sm_x.shape)\n",
    "sm_enc_x = enc(sm_x)\n",
    "print(\"enc_x:\", sm_enc_x.shape)\n",
    "assert sm_enc_x.shape[1]==sm_latent_dim\n",
    "assert sm_enc_x.shape[0]==nb_patches_per_img*batch_size\n",
    "sm_dec_x = dec(sm_enc_x)\n",
    "assert sm_dec_x.shape == (nb_patches_per_img*batch_size, 1, patch_size, patch_size), \"We should reconstruct each patch based on its latent representation\"\n",
    "print(\"dec_x:\", sm_dec_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer:\n",
    "Encoder maps x -> z_e (latent space)\n",
    "\n",
    "It quantizes the latent vectors to the nearest vector in the codebook, aka transform the encoder output into a discrete one-hot vector that is the index of the closest embedding vector z_e -> z_q\n",
    "\n",
    "Decoder maps z_q -> x_hat to reconstruct the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_num_embeddings = 5\n",
    "embedding = nn.Embedding(sm_num_embeddings, sm_latent_dim)\n",
    "ic(embedding.weight.data)\n",
    "embedding.weight.data.uniform_(-1/sm_num_embeddings, 1/sm_num_embeddings)\n",
    "ic(embedding.weight.data)\n",
    "\n",
    "z = sm_enc_x\n",
    "ic(z.shape)\n",
    "z_flattened = z.view(-1, sm_latent_dim)\n",
    "ic(z_flattened.shape)\n",
    "distances = torch.cdist(z_flattened, embedding.weight)\n",
    "ic(distances.shape)\n",
    "\n",
    "encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "ic(encoding_indices.shape)\n",
    "\n",
    "z_q = embedding(encoding_indices).view(z.shape)\n",
    "ic(z_flattened[3:6])\n",
    "ic(encoding_indices[3:6])\n",
    "ic(z_q[3:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Quantizer \n",
    "# wrap the logic into a nn.Module\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings:int, embedding_dim:int, dist_type: Literal['cosine','euclidean']='euclidean'):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "        self.dist_type = dist_type\n",
    "\n",
    "    def get_cosine_similarity_indices(self, z):\n",
    "        # Normalize the latent vectors and the embedding vectors to unit length\n",
    "        z_normalized = F.normalize(z, dim=-1)\n",
    "        embedding_normalized = F.normalize(self.embedding.weight, dim=-1)        \n",
    "        # Compute cosine similarity\n",
    "        similarity = torch.matmul(z_normalized, embedding_normalized.t())        \n",
    "        # Get the indices of the highest similarity (closest vectors)\n",
    "        encoding_indices = torch.argmax(similarity, dim=-1).unsqueeze(1)\n",
    "        return encoding_indices\n",
    "        \n",
    "    def get_euclid_dist_indices(self, z):\n",
    "        z_flattened = z.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(z_flattened, self.embedding.weight)\n",
    "        # Get the indices with smallest euclidean distance\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        return encoding_indices\n",
    "\n",
    "    def forward(self, z):\n",
    "        if self.dist_type =='cosine':        \n",
    "            encoding_indices = self.get_cosine_similarity_indices(z)\n",
    "        else:\n",
    "            encoding_indices = self.get_euclid_dist_indices(z)\n",
    "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
    "        return z_q, encoding_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vq = VectorQuantizer(num_embeddings, sm_latent_dim,dist_type='euclidean')\n",
    "sm_z_q, sm_z_q_idx = vq(sm_enc_x)\n",
    "assert sm_z_q.shape == sm_enc_x.shape\n",
    "sm_z_q[:4], sm_z_q_idx[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, latent_dim:int, num_embeddings:int, patch_size:int):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, patch_size=patch_size)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, _ = self.vector_quantizer(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, z, z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = VQVAE(latent_dim=sm_latent_dim, num_embeddings=sm_num_embeddings, patch_size=patch_size)\n",
    "x_recon, z, z_q = vqvae(sm_x)\n",
    "ic(x_recon.shape, z.shape, z_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "model = VQVAE(latent_dim, num_embeddings, patch_size=patch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z, z_q = model(x)\n",
    "            recon_loss = F.mse_loss(x_recon, x)\n",
    "            commitment_loss = F.mse_loss(z_q, z.detach())\n",
    "            loss = recon_loss + commitment_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "train(model, train_loader, optimizer, num_epochs)\n",
    "\n",
    "# Visualization of reconstructed images\n",
    "def show_reconstructed(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            x_recon, _, _ = model(x)\n",
    "            break\n",
    "    x = x.cpu().numpy()\n",
    "    x_recon = x_recon.cpu().numpy()\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(x[0][0], cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[1].imshow(x_recon[0][0], cmap='gray')\n",
    "    axes[1].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "\n",
    "show_reconstructed(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
