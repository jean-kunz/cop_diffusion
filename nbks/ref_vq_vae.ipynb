{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "from cop_diffusion.utils import save_model, load_model\n",
    "\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist():\n",
    "    train = datasets.FashionMNIST(root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ))\n",
    "    val = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = load_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    train = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    val = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def data_loaders(train_data, val_data, batch_size):\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "training_data, validation_data = load_cifar()\n",
    "#training_data, validation_data = load_fashion_mnist()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size=10\n",
    ")\n",
    "x_train_var = np.var(training_data.data / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data.min(), training_data.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(x):\n",
    "    x = make_grid(x.cpu().detach() + 0.5)\n",
    "    x = x.numpy()\n",
    "    ic(x.shape)\n",
    "    ic(np.transpose(x,(1,2,0)).shape)\n",
    "    fig = plt.imshow(np.transpose(x, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, _ =next(iter(training_loader))\n",
    "bx = bx.to(device)\n",
    "ic(bx.shape)\n",
    "display_image_grid(bx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet like layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(\n",
    "                in_dim, res_h_dim, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1, stride=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)] * n_res_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "#x = np.random.random_sample((3, 40, 40, 200))\n",
    "#x = torch.tensor(x).float()\n",
    "# test Residual Layer\n",
    "res = ResidualLayer(3, 3, 3).to(device)\n",
    "res_out = res(x)\n",
    "ic(res_out.shape)\n",
    "show_image(res_out,\"After resid. layer\")\n",
    "\n",
    "\n",
    "# test res stack\n",
    "for i in sorted(random.sample(list(range(1,101)), 5)):\n",
    "    res_stack = ResidualStack(3, 3, 3, i).to(device)\n",
    "    res_stack_out = res_stack(x)\n",
    "    ic(res_stack_out.shape);\n",
    "    show_image(res_stack_out, f\"After resid. stack {i} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ-VAE\n",
    "Operates on the whole image:\n",
    "1.\tEncoder: The encoder processes the entire image and converts it into a continuous latent representation.\n",
    "2.\tQuantization: This continuous latent representation is then mapped to a finite set of discrete latent vectors (the codebook).\n",
    "3.\tDecoder: The discrete latent vectors are then decoded back into the image.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the q_theta (z|x) network. Given a data sample x q_theta\n",
    "    maps to the latent space x -> z.\n",
    "\n",
    "    For a VQ VAE, q_theta outputs parameters of a categorical distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                h_dim, h_dim, kernel_size=kernel - 1, stride=stride - 1, padding=1\n",
    "            ),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "#x = np.random.random_sample((3, 40, 40, 200))\n",
    "#x = torch.tensor(x).float()\n",
    "\n",
    "# test encoder\n",
    "encoder = Encoder(in_dim=3, h_dim=3, n_res_layers=20, res_h_dim=10).to(device)\n",
    "encoder_out = encoder(x)\n",
    "ic(encoder_out.shape)\n",
    "#print(\"Encoder out shape:\", encoder_out.shape)\n",
    "show_image(encoder_out,\"After encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer\n",
    "\n",
    "Map continuous latent representation (as encoded) to a discrete one. \n",
    "\n",
    "A distance between z (encoded) and each embedding is computed: $$e_j (z - e)^2 = z^2 + e^2 - 2 e * z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30\n",
    "emb_dim = 10\n",
    "emb = nn.Embedding(vocab_size, emb_dim).to(device)\n",
    "emb.weight.data.uniform_(-1./vocab_size, 1./vocab_size,)\n",
    "ic(emb.weight.shape)\n",
    "# create a encoded z of emb_dim.\n",
    "\n",
    "enc = Encoder(in_dim=3, h_dim=emb_dim, n_res_layers=1, res_h_dim=3).to(device)\n",
    "z = enc(x)\n",
    "ic(z.shape)\n",
    "#reshape z -> (batch, height, width, channel) and flatten\n",
    "z = z.permute(0, 2, 3, 1).contiguous()\n",
    "ic(z.shape)\n",
    "z_flattened = z.view(-1, emb_dim)\n",
    "ic(z_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distance for each flattened item, its distance to each of embeddings, so it can take the closest one.\n",
    "dist = (\n",
    "    torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
    "    + torch.sum(emb.weight**2, dim=1)\n",
    "    - 2 * torch.matmul(z_flattened, emb.weight.t())\n",
    ")\n",
    "ic(dist.shape)\n",
    "assert dist.shape[1]==vocab_size\n",
    "\n",
    "min_encoding_indices = torch.argmin(dist, dim=1).to(device).unsqueeze(1)\n",
    "ic(min_encoding_indices)\n",
    "min_encodings = torch.zeros(min_encoding_indices.shape[0], vocab_size).to(device)\n",
    "min_encodings = F.one_hot(min_encoding_indices, num_classes=vocab_size)\n",
    "ic(min_encodings)\n",
    "# get quantized latent vectors\n",
    "#z_q = torch.matmul(min_encodings, emb.weight).view(z.shape)\n",
    "#emb(min_encoding_indices)\n",
    "min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(torch.tensor([0,1,3]).to(device))\n",
    "emb(min_encoding_indices).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_encoding_indices[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.vocab_size, 1.0 / self.vocab_size)\n",
    "\n",
    "    def get_device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete\n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, channel, height, width)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.emb_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = (\n",
    "            torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        #min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        #min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "        # get quantized latent vectors\n",
    "        #z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        z_q = self.embedding(min_encoding_indices)\n",
    "        z_q = z_q.view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean(\n",
    "            (z_q - z.detach()) ** 2\n",
    "        )\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        oh_encodings = F.one_hot(min_encoding_indices, num_classes=self.vocab_size).float()\n",
    "        e_mean = torch.mean(oh_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return loss, z_q, perplexity, oh_encodings, min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(in_dim=3, h_dim=emb_dim, n_res_layers=1, res_h_dim=3).to(device)\n",
    "z = enc(x)\n",
    "vq = VectorQuantizer(vocab_size=vocab_size, emb_dim=emb_dim, beta=.25).to(device)\n",
    "vq(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodes latent discrete representation to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the p_phi (x|z) network. Given a latent sample z p_phi\n",
    "    maps back to the original space z -> x.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, kernel_size=kernel - 1, stride=stride - 1, padding=1\n",
    "            ),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(\n",
    "                h_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                h_dim // 2, 3, kernel_size=kernel, stride=stride, padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "#x = np.random.random_sample((3, 40, 40, 200))\n",
    "#x = torch.tensor(x).float()\n",
    "\n",
    "# test decoder\n",
    "decoder = Decoder(in_dim=3, h_dim=3, n_res_layers=10, res_h_dim=15).to(device)\n",
    "decoder_out = decoder(encoder_out)\n",
    "print(\"Dncoder out shape:\", decoder_out.shape)\n",
    "ic(decoder_out.shape)\n",
    "show_image(decoder_out, \"Decoded image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        h_dim,\n",
    "        res_h_dim,\n",
    "        n_res_layers,\n",
    "        n_embeddings,\n",
    "        embedding_dim,\n",
    "        beta,\n",
    "        save_img_embedding_map=False,\n",
    "    ):\n",
    "        super(VQVAE, self).__init__()\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(\n",
    "            h_dim, embedding_dim, kernel_size=1, stride=1\n",
    "        )\n",
    "        # pass continuous latent vector through discretization bottleneck\n",
    "        self.vector_quantization = VectorQuantizer(n_embeddings, embedding_dim, beta)\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        if save_img_embedding_map:\n",
    "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
    "        else:\n",
    "            self.img_to_embedding_map = None\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        z_e = self.encoder(x)\n",
    "\n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "        embedding_loss, z_q, perplexity, _, _ = self.vector_quantization(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"original data shape:\", x.shape)\n",
    "            print(\"encoded data shape:\", z_e.shape)\n",
    "            print(\"recon data shape:\", x_hat.shape)\n",
    "            assert False\n",
    "\n",
    "        return embedding_loss, x_hat, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_updates = 5000\n",
    "n_hiddens = 128\n",
    "n_residual_hiddens = 32\n",
    "n_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "n_embeddings = 512\n",
    "beta = 0.25\n",
    "learning_rate = 3e-4\n",
    "log_interval = 100\n",
    "dataset = \"CIFAR10\"\n",
    "\n",
    "model_version = \"0.1.1\"\n",
    "model_name = \"vqvae\"\n",
    "\n",
    "model = VQVAE(\n",
    "    n_hiddens, n_residual_hiddens, n_residual_layers, n_embeddings, embedding_dim, beta\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "model.train()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size=batch_size\n",
    ")\n",
    "x_train_var = np.var(training_data.data / 255.0)\n",
    "ic(x_train_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"n_updates\": 0,\n",
    "    \"recon_errors\": [],\n",
    "    \"loss_train\": [],\n",
    "    \"perplexities\": [],\n",
    "}\n",
    "save: bool = True\n",
    "n_updates = 1001\n",
    "last_update = 0\n",
    "if last_update > 0:\n",
    "    model = load_model(\n",
    "        model_name=model_name, model_version=model_version, iter=last_update\n",
    "    )\n",
    "    from_update_nb = last_update + 1\n",
    "else:\n",
    "    from_update_nb = 0\n",
    "\n",
    "do_train:bool = True\n",
    "if do_train:\n",
    "    writer = SummaryWriter(\n",
    "        f\"../runs/{model_name}_{model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "    )\n",
    "    ex_x, ex_y = next(iter(training_loader))\n",
    "    writer.add_graph(model, (ex_x.to(device)), use_strict_trace=False)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "    split = \"train\"\n",
    "    with tqdm(\n",
    "                total=n_updates - from_update_nb,\n",
    "                desc=f\"Training update\",\n",
    "                unit=\"batch\",\n",
    "            ) as pbar:\n",
    "\n",
    "        for i in range(from_update_nb, n_updates):\n",
    "            (x, _) = next(iter(training_loader))\n",
    "\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embedding_loss, x_hat, perplexity = model(x)\n",
    "            recon_loss = torch.mean((x_hat - x) ** 2) / x_train_var\n",
    "            loss = recon_loss + embedding_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            results[\"recon_errors\"].append(recon_loss.cpu().detach().numpy())\n",
    "            results[\"perplexities\"].append(perplexity.cpu().detach().numpy())\n",
    "            results[\"loss_train\"].append(loss.cpu().detach().numpy())\n",
    "            results[\"n_updates\"] = i\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"update_nb\": i,\n",
    "                    \"train_loss\": f\"{results['loss_train'][-1]:.4f}\"\n",
    "                })\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                for name, kernel_weight in model.named_parameters():\n",
    "                    writer.add_histogram(name, kernel_weight, i)\n",
    "\n",
    "\n",
    "                inter_recon_errors = np.mean(results[\"recon_errors\"][-log_interval:])\n",
    "                inter_loss = np.mean(results[\"loss_train\"][-log_interval:])\n",
    "                inter_perplexity = np.mean(results[\"perplexities\"][-log_interval:])\n",
    "\n",
    "                writer.add_scalar(f\"{split} loss\", inter_loss, i)\n",
    "                writer.add_scalar(f\"{split} recon_errors\", inter_recon_errors, i)\n",
    "                writer.add_scalar(f\"{split} perplexity\", inter_perplexity, i)\n",
    "    if save:\n",
    "        save_model(model=model, model_name=model_name, model_version=model_version, iter=i)\n",
    "else:\n",
    "    model = load_model(model_name=model_name, model_version=model_version, iter=last_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(training_loader))\n",
    "display_image_grid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(data_loader, model):\n",
    "    (x, _) = next(iter(data_loader))\n",
    "    x = x.to(device)\n",
    "    vq_encoder_output = model.pre_quantization_conv(model.encoder(x))\n",
    "    _, z_q, _, _, e_indices = model.vector_quantization(vq_encoder_output)\n",
    "\n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x, x_recon, z_q, e_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_val_recon, z_q, e_indices = reconstruct(validation_loader, model)\n",
    "print(x_val.shape)\n",
    "display_image_grid(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(x_val_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
