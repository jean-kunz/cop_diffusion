{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MishaLaskin/vqvae/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_dim, res_h_dim, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1,\n",
    "                      stride=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)]*n_res_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "# test Residual Layer\n",
    "res = ResidualLayer(40, 40, 20)\n",
    "res_out = res(x)\n",
    "print('Res Layer out shape:', res_out.shape)\n",
    "# test res stack\n",
    "res_stack = ResidualStack(40, 40, 20, 3)\n",
    "res_stack_out = res_stack(x)\n",
    "print('Res Stack out shape:', res_stack_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the q_theta (z|x) network. Given a data sample x q_theta \n",
    "    maps to the latent space x -> z.\n",
    "\n",
    "    For a VQ VAE, q_theta outputs parameters of a categorical distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=kernel,\n",
    "                      stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=kernel,\n",
    "                      stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim, h_dim, kernel_size=kernel-1,\n",
    "                      stride=stride-1, padding=1),\n",
    "            ResidualStack(\n",
    "                h_dim, h_dim, res_h_dim, n_res_layers)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "# test encoder\n",
    "encoder = Encoder(40, 128, 3, 64)\n",
    "encoder_out = encoder(x)\n",
    "print('Encoder out shape:', encoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the p_phi (x|z) network. Given a latent sample z p_phi \n",
    "    maps back to the original space z -> x.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(h_dim, h_dim // 2,\n",
    "                               kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(h_dim//2, 3, kernel_size=kernel,\n",
    "                               stride=stride, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "# test decoder\n",
    "decoder = Decoder(40, 128, 3, 64)\n",
    "decoder_out = decoder(x)\n",
    "print('Dncoder out shape:', decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete \n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, channel, height, width)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(\n",
    "            min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "            torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return loss, z_q, perplexity, min_encodings, min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers,\n",
    "                 n_embeddings, embedding_dim, beta, save_img_embedding_map=False):\n",
    "        super(VQVAE, self).__init__()\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(\n",
    "            h_dim, embedding_dim, kernel_size=1, stride=1)\n",
    "        # pass continuous latent vector through discretization bottleneck\n",
    "        self.vector_quantization = VectorQuantizer(\n",
    "            n_embeddings, embedding_dim, beta)\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        if save_img_embedding_map:\n",
    "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
    "        else:\n",
    "            self.img_to_embedding_map = None\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "\n",
    "        z_e = self.encoder(x)\n",
    "\n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "        embedding_loss, z_q, perplexity, _, _ = self.vector_quantization(\n",
    "            z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "\n",
    "        if verbose:\n",
    "            print('original data shape:', x.shape)\n",
    "            print('encoded data shape:', z_e.shape)\n",
    "            print('recon data shape:', x_hat.shape)\n",
    "            assert False\n",
    "\n",
    "        return embedding_loss, x_hat, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "n_updates = 5000\n",
    "n_hiddens = 128\n",
    "n_residual_hiddens = 32\n",
    "n_residual_layers = 2\n",
    "embedding_dim=64\n",
    "n_embeddings=512\n",
    "beta=.25\n",
    "learning_rate=3e-4\n",
    "log_interval=50\n",
    "dataset='CIFAR10'\n",
    "\n",
    "\n",
    "\n",
    "model = VQVAE(n_hiddens, n_residual_hiddens,\n",
    "              n_residual_layers, n_embeddings, embedding_dim, beta).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    train = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(\n",
    "                                     (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                             ]))\n",
    "\n",
    "    val = datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                   (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                           ]))\n",
    "    return train, val\n",
    "\n",
    "def data_loaders(train_data, val_data, batch_size):\n",
    "\n",
    "    train_loader = DataLoader(train_data,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    val_loader = DataLoader(val_data,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "training_data, validation_data = load_cifar()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size)\n",
    "x_train_var = np.var(training_data.data / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, results, hyperparameters, timestamp):\n",
    "    SAVE_MODEL_PATH = os.getcwd() + '/results'\n",
    "\n",
    "    results_to_save = {\n",
    "        'model': model.state_dict(),\n",
    "        'results': results,\n",
    "        'hyperparameters': hyperparameters\n",
    "    }\n",
    "    torch.save(results_to_save,\n",
    "               SAVE_MODEL_PATH + '/vqvae_data_' + timestamp + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'n_updates': 0,\n",
    "    'recon_errors': [],\n",
    "    'loss_vals': [],\n",
    "    'perplexities': [],\n",
    "}\n",
    "save:bool = False\n",
    "n_updates=1\n",
    "for i in range(n_updates):\n",
    "    (x, _) = next(iter(training_loader))\n",
    "    \n",
    "    x = x.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    embedding_loss, x_hat, perplexity = model(x)\n",
    "    recon_loss = torch.mean((x_hat - x)**2) / x_train_var\n",
    "    loss = recon_loss + embedding_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results[\"recon_errors\"].append(recon_loss.cpu().detach().numpy())\n",
    "    results[\"perplexities\"].append(perplexity.cpu().detach().numpy())\n",
    "    results[\"loss_vals\"].append(loss.cpu().detach().numpy())\n",
    "    results[\"n_updates\"] = i\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print('Update #', i, 'Recon Error:',\n",
    "                np.mean(results[\"recon_errors\"][-log_interval:]),\n",
    "                'Loss', np.mean(results[\"loss_vals\"][-log_interval:]),\n",
    "                'Perplexity:', np.mean(results[\"perplexities\"][-log_interval:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
