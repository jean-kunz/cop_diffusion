{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "from cop_diffusion.utils import save_model, load_model\n",
    "\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist():\n",
    "    train = datasets.FashionMNIST(root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ))\n",
    "    val = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = load_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    train = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    val = datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def data_loaders(train_data, val_data, batch_size):\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "training_data, validation_data = load_cifar()\n",
    "#training_data, validation_data = load_fashion_mnist()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size=10\n",
    ")\n",
    "x_train_var = np.var(training_data.data / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data.min(), training_data.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(x):\n",
    "    x = make_grid(x.cpu().detach() + 0.5)\n",
    "    x = x.numpy()\n",
    "    ic(x.shape)\n",
    "    ic(np.transpose(x,(1,2,0)).shape)\n",
    "    fig = plt.imshow(np.transpose(x, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, _ =next(iter(training_loader))\n",
    "ic(bx.shape)\n",
    "display_image_grid(bx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_tensor, title: str):\n",
    "    # Convert the image tensor to a NumPy array\n",
    "    image_np = image_tensor.squeeze().detach().numpy()\n",
    "    image_np = np.transpose(image_np, (1, 2, 0))  # Rearrange dimensions to HWC\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(SimpleConv2D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize the weights of the convolution kernel\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add padding to the input tensor\n",
    "        if self.padding > 0:\n",
    "            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding))\n",
    "\n",
    "        # Get the dimensions of the input and kernel\n",
    "        batch_size, in_channels, input_height, input_width = x.size()\n",
    "        out_channels, _, kernel_height, kernel_width = self.weight.size()\n",
    "\n",
    "        # Calculate the dimensions of the output tensor\n",
    "        output_height = (input_height - kernel_height) // self.stride + 1\n",
    "        output_width = (input_width - kernel_width) // self.stride + 1\n",
    "\n",
    "        # Initialize the output tensor\n",
    "        output = torch.zeros(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "        # Perform the convolution operation\n",
    "        for i in range(0, input_height - kernel_height + 1, self.stride):\n",
    "            for j in range(0, input_width - kernel_width + 1, self.stride):\n",
    "                region = x[:, :, i:i+kernel_height, j:j+kernel_width]\n",
    "                output[:, :, i // self.stride, j // self.stride] = torch.sum(region * self.weight, dim=(1, 2, 3)) + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the SimpleConv2D layer\n",
    "conv_layer = SimpleConv2D(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "# Apply the convolution\n",
    "image_tensor = bx[0].unsqueeze(0)\n",
    "show_image(image_tensor, \"Original\")\n",
    "\n",
    "output = conv_layer(image_tensor)\n",
    "\n",
    "# Convert tensors to NumPy arrays for displaying the input and output\n",
    "show_image(output, \"Convoluted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the convolution kernel for vertical line detection\n",
    "vertical_line_kernel = torch.tensor(\n",
    "    [[[[1, 0, -1], [1, 0, -1], [1, 0, -1]]]], dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Replicate the kernel for each input channel (R, G, B)\n",
    "vertical_line_kernel = vertical_line_kernel.repeat(3, 1, 1, 1)\n",
    "\n",
    "# Define a convolution layer with the pre-defined kernel\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=3, out_channels=3, kernel_size=3, padding=1, bias=False, groups=3\n",
    ")\n",
    "conv.weight = nn.Parameter(vertical_line_kernel)\n",
    "\n",
    "# Apply the convolution\n",
    "image_tensor = bx[0].unsqueeze(0)\n",
    "show_image(image_tensor, \"Original\")\n",
    "output = conv(image_tensor)\n",
    "\n",
    "show_image(output, \"Convolved vertical lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MishaLaskin/vqvae/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(\n",
    "                in_dim, res_h_dim, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1, stride=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)] * n_res_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "# test Residual Layer\n",
    "res = ResidualLayer(40, 40, 20)\n",
    "res_out = res(x)\n",
    "print(\"Res Layer out shape:\", res_out.shape)\n",
    "# test res stack\n",
    "res_stack = ResidualStack(40, 40, 20, 3)\n",
    "res_stack_out = res_stack(x)\n",
    "print(\"Res Stack out shape:\", res_stack_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the q_theta (z|x) network. Given a data sample x q_theta\n",
    "    maps to the latent space x -> z.\n",
    "\n",
    "    For a VQ VAE, q_theta outputs parameters of a categorical distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                h_dim, h_dim, kernel_size=kernel - 1, stride=stride - 1, padding=1\n",
    "            ),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "# test encoder\n",
    "encoder = Encoder(40, 128, 3, 64)\n",
    "encoder_out = encoder(x)\n",
    "print(\"Encoder out shape:\", encoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the p_phi (x|z) network. Given a latent sample z p_phi\n",
    "    maps back to the original space z -> x.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, kernel_size=kernel - 1, stride=stride - 1, padding=1\n",
    "            ),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(\n",
    "                h_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                h_dim // 2, 3, kernel_size=kernel, stride=stride, padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "x = np.random.random_sample((3, 40, 40, 200))\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "# test decoder\n",
    "decoder = Decoder(40, 128, 3, 64)\n",
    "decoder_out = decoder(x)\n",
    "print(\"Dncoder out shape:\", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete\n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "\n",
    "        z (continuous) -> z_q (discrete)\n",
    "\n",
    "        z.shape = (batch, channel, height, width)\n",
    "\n",
    "        quantization pipeline:\n",
    "\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W,C)\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = (\n",
    "            torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean(\n",
    "            (z_q - z.detach()) ** 2\n",
    "        )\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return loss, z_q, perplexity, min_encodings, min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        h_dim,\n",
    "        res_h_dim,\n",
    "        n_res_layers,\n",
    "        n_embeddings,\n",
    "        embedding_dim,\n",
    "        beta,\n",
    "        save_img_embedding_map=False,\n",
    "    ):\n",
    "        super(VQVAE, self).__init__()\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(\n",
    "            h_dim, embedding_dim, kernel_size=1, stride=1\n",
    "        )\n",
    "        # pass continuous latent vector through discretization bottleneck\n",
    "        self.vector_quantization = VectorQuantizer(n_embeddings, embedding_dim, beta)\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        if save_img_embedding_map:\n",
    "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
    "        else:\n",
    "            self.img_to_embedding_map = None\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        z_e = self.encoder(x)\n",
    "\n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "        embedding_loss, z_q, perplexity, _, _ = self.vector_quantization(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"original data shape:\", x.shape)\n",
    "            print(\"encoded data shape:\", z_e.shape)\n",
    "            print(\"recon data shape:\", x_hat.shape)\n",
    "            assert False\n",
    "\n",
    "        return embedding_loss, x_hat, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_updates = 5000\n",
    "n_hiddens = 128\n",
    "n_residual_hiddens = 32\n",
    "n_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "n_embeddings = 512\n",
    "beta = 0.25\n",
    "learning_rate = 3e-4\n",
    "log_interval = 100\n",
    "dataset = \"CIFAR10\"\n",
    "\n",
    "model_version = \"0.1\"\n",
    "model_name = \"vqvae\"\n",
    "\n",
    "model = VQVAE(\n",
    "    n_hiddens, n_residual_hiddens, n_residual_layers, n_embeddings, embedding_dim, beta\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "model.train()\n",
    "training_loader, validation_loader = data_loaders(\n",
    "    training_data, validation_data, batch_size=batch_size\n",
    ")\n",
    "x_train_var = np.var(training_data.data / 255.0)\n",
    "ic(x_train_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"n_updates\": 0,\n",
    "    \"recon_errors\": [],\n",
    "    \"loss_train\": [],\n",
    "    \"perplexities\": [],\n",
    "}\n",
    "save: bool = True\n",
    "n_updates = 10001\n",
    "last_update = 10000\n",
    "if last_update > 0:\n",
    "    model = load_model(\n",
    "        model_name=model_name, model_version=model_version, iter=last_update\n",
    "    )\n",
    "    from_update_nb = last_update + 1\n",
    "else:\n",
    "    from_update_nb = 0\n",
    "\n",
    "do_train:bool = False\n",
    "if do_train:\n",
    "    writer = SummaryWriter(\n",
    "        f\"../runs/{model_name}_{model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "    )\n",
    "    ex_x, ex_y = next(iter(training_loader))\n",
    "    writer.add_graph(model, (ex_x.to(device)), use_strict_trace=True)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "    split = \"train\"\n",
    "    with tqdm(\n",
    "                total=n_updates - from_update_nb,\n",
    "                desc=f\"Training update\",\n",
    "                unit=\"batch\",\n",
    "            ) as pbar:\n",
    "\n",
    "        for i in range(from_update_nb, n_updates):\n",
    "            (x, _) = next(iter(training_loader))\n",
    "\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embedding_loss, x_hat, perplexity = model(x)\n",
    "            recon_loss = torch.mean((x_hat - x) ** 2) / x_train_var\n",
    "            loss = recon_loss + embedding_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            results[\"recon_errors\"].append(recon_loss.cpu().detach().numpy())\n",
    "            results[\"perplexities\"].append(perplexity.cpu().detach().numpy())\n",
    "            results[\"loss_train\"].append(loss.cpu().detach().numpy())\n",
    "            results[\"n_updates\"] = i\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"update_nb\": i,\n",
    "                    \"train_loss\": f\"{results['loss_train'][-1]:.4f}\"\n",
    "                })\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                for name, weight in model.named_parameters():\n",
    "                    writer.add_histogram(name, weight, i)\n",
    "\n",
    "\n",
    "                inter_recon_errors = np.mean(results[\"recon_errors\"][-log_interval:])\n",
    "                inter_loss = np.mean(results[\"loss_train\"][-log_interval:])\n",
    "                inter_perplexity = np.mean(results[\"perplexities\"][-log_interval:])\n",
    "\n",
    "                writer.add_scalar(f\"{split} loss\", inter_loss, i)\n",
    "                writer.add_scalar(f\"{split} recon_errors\", inter_recon_errors, i)\n",
    "                writer.add_scalar(f\"{split} perplexity\", inter_perplexity, i)\n",
    "    if save:\n",
    "        save_model(model=model, model_name=model_name, model_version=model_version, iter=i)\n",
    "else:\n",
    "    model = load_model(model_name=model_name, model_version=model_version, iter=last_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(training_loader))\n",
    "display_image_grid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(data_loader, model):\n",
    "    (x, _) = next(iter(data_loader))\n",
    "    x = x.to(device)\n",
    "    vq_encoder_output = model.pre_quantization_conv(model.encoder(x))\n",
    "    _, z_q, _, _, e_indices = model.vector_quantization(vq_encoder_output)\n",
    "\n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x, x_recon, z_q, e_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_val_recon, z_q, e_indices = reconstruct(validation_loader, model)\n",
    "print(x_val.shape)\n",
    "display_image_grid(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(x_val_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
