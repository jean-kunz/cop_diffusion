{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp diff_scratch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env PYDEVD_DISABLE_FILE_VALIDATION=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "#from diffusers import DDPMScheduler, UNet2DModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from cop_diffusion.utils import save_model, load_model\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"mnist/\", train=True, download=True, transform=transform\n",
    ")\n",
    "val_dataset = datasets.MNIST(root=\"mnist/\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "oh_y = torch.nn.functional.one_hot(y)\n",
    "ic(\"Input shape:\", x.shape)\n",
    "ic(\"Labels:\", y)\n",
    "plt.imshow(torchvision.utils.make_grid(x)[0], cmap=\"Greys\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corruption process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amount = torch.linspace(0, 1, x.shape[0])\n",
    "amount = amount.view(-1, 1, 1, 1)\n",
    "mean = 0.0\n",
    "std = 1.0\n",
    "noise = torch.rand_like(x)*std + mean\n",
    "x_noisy = (1-amount)*x + amount*noise\n",
    "#x_noisy = amount*noise + x\n",
    "\n",
    "ic(amount.squeeze())\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(50, 5))\n",
    "axs[0].set_title(\"100% of X\")\n",
    "axs[0].imshow(torchvision.utils.make_grid(x)[0], cmap=\"Greys\")\n",
    "\n",
    "axs[1].set_title(f\"1-Amount  of X\")\n",
    "axs[1].imshow(torchvision.utils.make_grid(x*(1-amount))[0], cmap=\"Greys\")\n",
    "\n",
    "axs[2].set_title(f\"Amount  of noise\")\n",
    "axs[2].imshow(torchvision.utils.make_grid(noise*amount)[0], cmap=\"Greys\")\n",
    "\n",
    "\n",
    "axs[3].set_title(f\"Noisy image: 1- amount of x, amount of noise\")\n",
    "axs[3].imshow(torchvision.utils.make_grid(x_noisy)[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise scheduling\n",
    "\n",
    "- Beta $\\beta$, control the noise added at each step. Beta is the noise schedule, the variance of noised added (between 0..1) at each t\n",
    "- Alpha $\\alpha$, control the noise removed at each step $\\alpha_t = 1-\\beta_t$. This is the **signal retention** \n",
    "\n",
    "Choosing the beta schedule is key in the process. It can be\n",
    "- linear: increase linearly over timestep\n",
    "- cosine: follow a cosine function, start slowly, increase rapidly, slow down again. Often better perf. \n",
    "- quadratic or other non linear schedules. \n",
    "\n",
    "#### Forward\n",
    "\n",
    "Gradually add noise to image over serie of timesteps \n",
    "$x_t = \\sqrt{\\alpha_{\\text{cumprod}t}} \\cdot x_0 + \\sqrt{1 - \\alpha_{\\text{cumprod}_t}} \\cdot \\epsilon$\n",
    "\n",
    "- $\\epsilon$ : the gaussian noise\n",
    "- $x_0$: original image\n",
    "- $\\alpha = 1 - \\beta $\n",
    "- $\\alpha_{cumprod_t}$ cumulative product up to time step t. Represent how much of the original image remains after t steps.\n",
    "\n",
    "\n",
    "#### Reverse diffusion (Denoising)\n",
    "\n",
    "To reverse diffusion we could do it in 2 different ways:\n",
    "\n",
    "a. Estimate directly $x_0$ from predicted noise\n",
    "\n",
    "Estimate what a clean desk would be without the mess. Mentally subtract the mess. So we directly estimate the original image from the noisy one at timestep t. In diffusion model, we don't use it. \n",
    "$\\hat{x}0 = \\frac{x_t - \\sqrt{1 - \\alpha{\\text{cumprod}t}} \\cdot \\epsilon{\\theta}(x_t, t)}{\\sqrt{\\alpha_{\\text{cumprod}_t}}}$\n",
    "\n",
    "- $\\epsilon{\\theta}(x_t,t)$: the model prediction of the noise added at timestep t\n",
    "- $\\alpha{\\text{cumprod}t}$: how much of the original signal remains. \n",
    " \n",
    "\n",
    "b. Iterate\n",
    "\n",
    "Transition to a less messy desk, by using smooth transitions. \n",
    "Iteratively remove the noise added in forward process to reconstruct original image from noisy image. Trained model predict the noise.\n",
    "\n",
    "It incorporate posterior variance randomness that prevent exact reconstruction and avoid collapse. \n",
    "\n",
    "$x_{t-1} = \\sqrt{\\frac{1}{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\alpha_{\\text{cumprod}t}}} \\cdot \\epsilon{\\theta}(x_t, t) \\right) + \\sigma_t \\cdot \\epsilon$\n",
    "\n",
    "- $\\epsilon$ is some random noise added.\n",
    "- $\\sigma$ is $\\frac{1-\\alpha_{cumprod_{t-1}}}{1-\\alpha_{cumprod_{t}}}{\\beta_t}$\n",
    "\n",
    "##### Why directly estimating $x_0$ isn't used\n",
    "\n",
    "it would require the model to  handle a wider range of outputs, increasig complexity and variablity. With iterative noise prediction, the output space is consistent and simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMScheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps: int=1000, beta_start:float=1e-4, beta_end:float=0.02, device:str='cpu'):\n",
    "        super().__init__()\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.device = device\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_time_steps, requires_grad=False).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        # cumulative product of alphas upto time t, quantiies the amount of info retained at time t in the forward diffusion process\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).requires_grad_(False)\n",
    "        # special case: t=0, no previous timestep\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]).to(device), self.alphas_cumprod[:-1]]).requires_grad_(False)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alphas_cumprod).to(device)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alphas_cumprod).to(device)\n",
    "        self.sqrt_reciprocal_alphas = torch.sqrt(1.0 / self.alphas).to(device)\n",
    "\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "\n",
    "    def add_noise_step(self, x, t) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Gradually add noise to the image\n",
    "        \"\"\"\n",
    "        sqrt_alpha_cum_prod_t = self.sqrt_alpha_cumprod[t].view(-1,1,1,1)\n",
    "        sqrt_one_minus_alpha_cum_prod_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1,1,1,1)\n",
    "        noise = torch.randn_like(x).to(self.device)\n",
    "        noisy_x = sqrt_alpha_cum_prod_t * x + sqrt_one_minus_alpha_cum_prod_t * noise\n",
    "        return noisy_x, noise\n",
    "\n",
    "    def denoise_step(self, pred_noise, x_noisy, t):\n",
    "        \"\"\" estimage previous image $x_{t-1}$\n",
    "        As it incorporate posterior variance, randomnsess that prevent exact reconstruction.\n",
    "        \"\"\"\n",
    "        t = t-1\n",
    "        # we get value at time t\n",
    "        sqrt_reciprocal_alpha_t = self.sqrt_reciprocal_alphas[t].view(-1,1,1,1)\n",
    "        sqrt_one_minus_alpha_cum_prod_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1,1,1,1)\n",
    "        beta_t = self.betas[t].view(-1,1,1,1)\n",
    "        posterior_variance_t = self.posterior_variance[t].view(-1,1,1,1)\n",
    "\n",
    "        # Compute the mean of the posterior q(x_{t-1} | x_t, x_0)\n",
    "        # Using the DDPM reverse process mean formula\n",
    "        #pred_mean = sqrt_reciprocal_alpha_t * (x_noisy * beta_t * pred_noise) / sqrt_one_minus_alpha_cum_prod_t\n",
    "        pred_mean = sqrt_reciprocal_alpha_t * (x_noisy - (beta_t / sqrt_one_minus_alpha_cum_prod_t) * pred_noise)\n",
    "\n",
    "        # sample noise for the next step to avoid collaps of model.\n",
    "        z = torch.randn_like(x_noisy).to(self.device)\n",
    "        x_prev = pred_mean + torch.sqrt(posterior_variance_t)* z\n",
    "        return x_prev\n",
    "\n",
    "\n",
    "scheduler = DDPMScheduler(num_time_steps=500, device=device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "t = torch.tensor([0,50, 100, 150, 200,300,400,499]).to(device)\n",
    "assert t.shape[0]==x.shape[0], \"it should be the same nb of examples\"\n",
    "x = x.to(device)\n",
    "x_noisy, noise = scheduler.add_noise_step(x, t)\n",
    "ic(x_noisy.shape, noise.shape)\n",
    "\n",
    "plt.imshow(torchvision.utils.make_grid(x.cpu())[0], cmap=\"Greys\")\n",
    "plt.title(\"X\")\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(torchvision.utils.make_grid(x_noisy.cpu())[0], cmap=\"Greys\")\n",
    "# plt.title(\"X (noisy)\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(torchvision.utils.make_grid(noise.cpu())[0], cmap=\"Greys\")\n",
    "# plt.title(\"noise\")\n",
    "# plt.show()\n",
    "\n",
    "# denoised_x = scheduler.denoise_step(noise, x_noisy, t)\n",
    "# plt.imshow(torchvision.utils.make_grid(denoised_x.cpu())[0], cmap=\"Greys\")\n",
    "# plt.title(\"denoised_x\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "no_noise_denoised_x = scheduler.denoise_step(torch.zeros_like(noise), x,t)\n",
    "plt.imshow(torchvision.utils.make_grid(no_noise_denoised_x.cpu())[0], cmap=\"Greys\")\n",
    "plt.title(\"denoised_x with no noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model to learn noise\n",
    "\n",
    "We’d like a model that takes in a 28px noisy images and outputs a prediction of the same shape.\n",
    "\n",
    "**UNet** consists of a ‘constricting path’ through which data is compressed down and an ‘expanding path’ through which it expands back up to the original dimension (similar to an autoencoder) but also features skip connections that allow for information and gradients to flow across at different levels.\n",
    "\n",
    "![UNET](../res/unet.png)\n",
    "\n",
    "\n",
    "### See convolution_101.ipynb for better understanding of convolutions\n",
    "\n",
    "![Convolution_101](./convolution_101.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_channel(input, channel_nb:int=0, info=\"\"):\n",
    "    '''You must provide on image of shape (batch, channel, height, width)convolved on many channels'''\n",
    "    title = f\"channel {channel_nb}, info: {info}\"\n",
    "    img = input[channel_nb].cpu().detach().numpy()\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels=1\n",
    "out_channels=1\n",
    "ic(x.shape)\n",
    "c0 = nn.Conv2d(in_channels, 32, kernel_size=5, padding=2).to(device)(x)\n",
    "ic(c0.shape);\n",
    "downscale = nn.MaxPool2d(2).to(device)\n",
    "cd0 = downscale(c0)\n",
    "ic(cd0.shape)\n",
    "show_channel(cd0[0],channel_nb=0, info='conv 0 -> downscale' )\n",
    "show_channel(cd0[0],channel_nb=31, info='conv 0 -> downscale' )\n",
    "\n",
    "\n",
    "c1 = nn.Conv2d(32, 64, kernel_size=5, padding=2).to(device)(cd0)\n",
    "cd1 = downscale(c1)\n",
    "ic(cd1.shape)\n",
    "show_channel(cd1[0], channel_nb=63,info='conv 1 -> downscale' )\n",
    "\n",
    "# Decoding part\n",
    "upscale = nn.Upsample(scale_factor=2).to(device)\n",
    "u0 = upscale(cd1)\n",
    "ic(u0.shape)\n",
    "show_channel(u0[0],channel_nb=0, info='upscale')\n",
    "uc0 = nn.Conv2d(64, 64, kernel_size=5, padding=2).to(device)(u0)\n",
    "show_channel(uc0[:,0],channel_nb=0, info='upscale -> conv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_timesteps=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create a matrix of [max_timesteps, embedding_dim] with positional encodings\n",
    "        pe = torch.zeros(max_timesteps, embedding_dim)\n",
    "        position = torch.arange(0, max_timesteps, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: Tensor of shape (batch_size,) containing timesteps.\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, embedding_dim).\n",
    "        \"\"\"\n",
    "        return self.pe[t]\n",
    "\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self, ctx_nb_feats=10, embedding_dim=128, dropout_prob=0.2):\n",
    "        super(SimpleDiffusionModel, self).__init__()\n",
    "        self.ctx_nb_feats = ctx_nb_feats\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout_prob / 2)\n",
    "        # Embedding for digit labels\n",
    "        self.label_embedding = nn.Embedding(ctx_nb_feats, embedding_dim)\n",
    "\n",
    "        # Positional encoding for timesteps\n",
    "        self.time_embedding = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        # Fully connected layer to project combined embeddings\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64 * 28 * 28)\n",
    "\n",
    "        # U-Net-like architecture with skip connections\n",
    "        self.encoder1 = nn.Conv2d(1 + 64, 128, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout2d(dropout_prob)\n",
    "        self.encoder2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.dropout2 = nn.Dropout2d(dropout_prob)\n",
    "        self.decoder1 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dropout3 = nn.Dropout2d(dropout_prob)\n",
    "        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dropout4 = nn.Dropout2d(dropout_prob)\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        self.relu_enc1 = nn.ReLU()\n",
    "        self.relu_enc2 = nn.ReLU()\n",
    "        self.relu_dec1 = nn.ReLU()\n",
    "        self.relu_dec2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t, c):\n",
    "        # Embed labels\n",
    "        label_emb = self.embedding_dropout(self.label_embedding(c))  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Embed timesteps\n",
    "        time_emb = self.embedding_dropout(self.time_embedding(t))  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Combine label and time embeddings\n",
    "        combined_emb = label_emb + time_emb  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Project combined embeddings to match image dimensions\n",
    "        emb_proj = self.fc1(combined_emb)  # (batch_size, 64*28*28)\n",
    "        emb_proj = emb_proj.view(-1, 64, 28, 28)  # (batch_size, 64, 28, 28)\n",
    "\n",
    "        # Concatenate embeddings with the input image\n",
    "        x = torch.cat([x, emb_proj], dim=1)  # (batch_size, 1+64, 28, 28)\n",
    "\n",
    "        # Encoder\n",
    "        #x = self.relu_enc1(self.encoder1(x))\n",
    "        x = self.dropout1(self.relu_enc1(self.encoder1(x)))\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.dropout2(self.relu_enc2(self.encoder2(x)))\n",
    "        skip2 = x\n",
    "\n",
    "        # Decoder\n",
    "        x = self.dropout3(self.relu_dec1(self.decoder1(x+skip2)))\n",
    "        x = self.dropout4(self.relu_dec2(self.decoder2(x+skip1)))\n",
    "\n",
    "        # Final output\n",
    "        x = self.final_conv(x)  # No activation\n",
    "        return x  # Predicted noise\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "model = SimpleDiffusionModel(ctx_nb_feats=9).to(device)\n",
    "#x = torch.rand(8, 1, 28, 28)\n",
    "t = torch.zeros(x.shape[0]).long().to(device)\n",
    "#oh_y = oh_y.to(device)\n",
    "x = x.to(device)\n",
    "y = y.long().to(device)\n",
    "ic(t.shape, y.shape)\n",
    "ux = model(x,t, y)\n",
    "ic(ux.shape, x.shape)\n",
    "ic(torchvision.utils.make_grid(x.cpu())[0].shape)\n",
    "plt.imshow(torchvision.utils.make_grid(x.cpu())[0], cmap=\"Greys\")\n",
    "plt.show()\n",
    "plt.imshow(torchvision.utils.make_grid(ux.cpu())[0], cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SimpleDifusionModel UNet model (DDPM)\n",
    "\n",
    "https://github.com/nickd16/Diffusion-Models-from-Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_dataset, scheduler, num_time_steps:int=1000, batch_size:int=256, device:torch.device=\"cpu\"):\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        criterion = nn.MSELoss(reduction='mean')#.to(device)\n",
    "        total_val_loss = 0\n",
    "        for batch_nb, (x, y) in enumerate(val_dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            b = x.size(0)\n",
    "            t = torch.randint(0, num_time_steps,(b,)).to(device)\n",
    "            x_noisy, noise  = scheduler.add_noise_step(x,t )\n",
    "            x_noisy= x_noisy\n",
    "\n",
    "            # we may exhaust the loader and have a smaller batch\n",
    "            output = model(x_noisy,t,y)\n",
    "            val_loss = criterion(output, noise)\n",
    "            total_val_loss += val_loss.item()\n",
    "        mean_loss = total_val_loss / len(val_dataloader)\n",
    "        model.train()\n",
    "        return mean_loss\n",
    "\n",
    "#ic(validate_model(model, val_dataset, scheduler, device=device))\n",
    "#ic(\"should be about the same loss:\",validate_model(model, train_dataset, scheduler, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model_id:str, use_tiny_dataset:bool)->str:\n",
    "    model_name = f\"mnist_{'tiny_' if use_tiny_dataset else ''}{model_id}\"\n",
    "    return model_name\n",
    "\n",
    "def train_model(get_new_model,  train_dataset, val_dataset=None, scheduler=None, device='cpu',\n",
    "                num_time_steps=1000, n_epoch=5, batch_size=32, lr=1e-4,\n",
    "                model_id='model', model_version='0.1', log_interval=50,\n",
    "                last_epoch=0, use_tiny_dataset=False):\n",
    "    if use_tiny_dataset:\n",
    "        random_indices = torch.randperm(len(train_dataset))[:2]\n",
    "        train_tiny_dataset = torch.utils.data.Subset(train_dataset, random_indices)\n",
    "        train_dataloader = DataLoader(train_tiny_dataset, batch_size=5, shuffle=True)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model_name = get_model_name(model_id, use_tiny_dataset=use_tiny_dataset)\n",
    "    if last_epoch > 0:\n",
    "        model = load_model(model_name=model_name, model_version=model_version, iter=last_epoch).to(device)\n",
    "        from_epoch_nb = last_epoch + 1\n",
    "    else:\n",
    "        model = get_new_model().to(device)\n",
    "        from_epoch_nb = 0\n",
    "\n",
    "    writer = SummaryWriter(f\"../runs/{model_name}_{model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\")\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    nb_batches = len(train_dataloader)\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=(n_epoch + 1 - from_epoch_nb) * nb_batches, desc=f\"Training update\", unit=\"batch\") as pbar:\n",
    "        for epoch in range(from_epoch_nb, n_epoch + 1):\n",
    "            total_loss = 0\n",
    "            for batch_nb, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.long().to(device)\n",
    "                b = x.size(0)\n",
    "                # timestep starts at 0\n",
    "                t = torch.randint(0, num_time_steps-1, (b,)).to(device)\n",
    "                x_noisy, noise = scheduler.add_noise_step(x, t)\n",
    "                x_noisy = x_noisy.to(device)\n",
    "\n",
    "                output = model(x_noisy, t, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(output, noise)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                # add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                i = (epoch * nb_batches) + batch_nb\n",
    "                writer.add_scalar(\"train loss\", loss.item(), i)\n",
    "                writer.add_histogram(\"pred_noise\", output, i)\n",
    "                writer.add_histogram(\"real_noise\", noise, i)\n",
    "                if i % log_interval == 0:\n",
    "                    #ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "                    for name, kernel_weight in model.named_parameters():\n",
    "                        if kernel_weight.numel() > 0:\n",
    "                            writer.add_histogram(name, kernel_weight, i)\n",
    "                        if kernel_weight.grad is not None:\n",
    "                            g = kernel_weight.grad\n",
    "                            writer.add_histogram(f\"{name}.grad\", g, i)\n",
    "                            g_norm = g.data.norm(2)\n",
    "                            writer.add_histogram(f\"{name}.grad_norm\", g_norm, i)\n",
    "                            # add proportion of gradient compared to weights (karpathy)\n",
    "                            ud = (lr * g).std() / kernel_weight.data.std().log10().item()\n",
    "                            writer.add_scalar(f\"{name}.grad_std_per_weight\", ud, i)\n",
    "\n",
    "                    if not use_tiny_dataset :\n",
    "                        val_loss = validate_model(model=model, val_dataset=val_dataset, scheduler=scheduler, num_time_steps=num_time_steps, batch_size=batch_size, device=device)\n",
    "                        writer.add_scalar(\"val loss\", val_loss, i)\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"batch_nb\": batch_nb,\n",
    "                    \"train_loss\": f\"{loss.item():.4f}\",\n",
    "                })\n",
    "\n",
    "            if not use_tiny_dataset :\n",
    "                save_model(model=model, model_name=model_name, model_version=model_version, iter=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 1000\n",
    "scheduler = DDPMScheduler(num_time_steps=num_time_steps, beta_end=0.01, device=device)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "use_tiny_dataset = False\n",
    "model_id = \"SimpleDiffusionModel\"\n",
    "model_version = \"0.5\"\n",
    "use_tiny_dataset = False\n",
    "n_epochs = 5\n",
    "last_epoch_nb = 0\n",
    "\n",
    "\n",
    "if True:\n",
    "    torch.manual_seed(42)\n",
    "    train_model(\n",
    "        get_new_model=lambda: SimpleDiffusionModel(ctx_nb_feats=10).to(device) ,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_time_steps=num_time_steps,\n",
    "        lr=1e-4,\n",
    "        n_epoch=n_epochs,\n",
    "        log_interval=200,\n",
    "        last_epoch=last_epoch_nb,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        model_id=model_id,\n",
    "        model_version=model_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "- after 5 epochs there are some results. After it only returns grey value with no noise \n",
    "- with this model with a tiny dataset (5 examples) it cannot go lower than 0.45 after 1000 or 5000 epochs. It should overfit to 0 loss\n",
    "- Enhance the model by adding capacity and by checking it overfit to 0 loss with a very small dataset (like 5 samples)\n",
    "\n",
    "### Ideas\n",
    "\n",
    "when training, train with all timesteps. !!!!\n",
    "should i use batch norm in model.\n",
    "try different learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "#n_sample = 5\n",
    "h,w = 28,28\n",
    "\n",
    "def show_images(imgs_tensor):\n",
    "    imgs_tensor = (imgs_tensor * 0.5) + 0.5  # Assuming normalization was to [-1, 1]\n",
    "    grid = torchvision.utils.make_grid(imgs_tensor.cpu())\n",
    "    plt.imshow(grid.permute(1, 2, 0)[:, :, 0], cmap=\"Greys\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def generate(model: nn.Module, scheduler:nn.Module, numbers_to_gen:list[int]=[0,1,2,3,4,5,6,7,8,9], show_timesteps:list[int]=[900,800,700,500,100,10,5,4,3,2,1]):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ctx = torch.tensor(numbers_to_gen).to(device)\n",
    "        n_sample = ctx.size(0)\n",
    "        samples = torch.randn(n_sample,1,h,w).to(device)\n",
    "        show_images(samples)\n",
    "        ic(samples.size(), samples.mean(), samples.std())\n",
    "        with tqdm(\n",
    "            total=num_time_steps,\n",
    "            desc=f\"Denoise\",\n",
    "            unit=\"timestep\",\n",
    "        ) as pbar:\n",
    "            for i in range(num_time_steps-1, 0,-1):\n",
    "                t = torch.tensor([i]*n_sample).to(device)\n",
    "                pred_noise = model(samples,t,ctx)\n",
    "                samples = scheduler.denoise_step(pred_noise=pred_noise, x_noisy=samples,t=t)\n",
    "                # From t=900, samples.mean() is nan !!!!!\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                            \"timestep\": i,\n",
    "                        })\n",
    "\n",
    "                if i in show_timesteps: #or i >995:\n",
    "                    #show_images(pred_noise)\n",
    "                    ic(pred_noise.mean(), pred_noise.std())\n",
    "                    ic(samples.mean(), samples.std())\n",
    "                    show_images(samples)\n",
    "\n",
    "model_name = get_model_name(model_id, use_tiny_dataset=use_tiny_dataset)\n",
    "model = load_model(\n",
    "    model_name=model_name, model_version=model_version, iter=n_epochs\n",
    ").to(device)\n",
    "\n",
    "generate(model, numbers_to_gen=[0,1,2,3,4,5,6,7,8,9], scheduler=scheduler, show_timesteps=[1000,996,995, 800,700,600,50,20,10,4,3,2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "x, y = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    print(f'{module}:')\n",
    "    print(f\"min:{output.min()}, mean: {output.mean()}, std: {output.std()}\")\n",
    "    show_images(output)\n",
    "\n",
    "\n",
    "# Register hooks for each layer in nn.Sequential\n",
    "model = DoubleConv(1,1)\n",
    "for layer in model.conv:\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "imgs = model(x)\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout:float=0.5):\n",
    "        super(ResidualDoubleConv, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.double_conv(x) + self.residual_conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualDoubleConv(1,1,dropout=0.1)\n",
    "model.residual_conv.register_forward_hook(hook_fn)\n",
    "\n",
    "imgs = model(x)\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, in_channel_divider:int=8):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // in_channel_divider, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // in_channel_divider, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.attention = None  #to be able to access it with a hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        key = self.key(x).view(batch_size, -1, width * height)\n",
    "        attention = torch.bmm(query, key)\n",
    "        self.attention = F.softmax(attention, dim=-1)\n",
    "        value = self.value(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(value, self.attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = self.gamma * out + x\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "def atn_hook_fn(module, input, output):\n",
    "    print(f'{module}:')\n",
    "    attention_value = module.attention\n",
    "    print(f\"min:{attention_value.min()}, mean: {attention_value.mean()}, std: {attention_value.std()}\")\n",
    "    print(f\"{attention_value.shape}\")\n",
    "# in_chann\n",
    "rc = ResidualDoubleConv(1,1,dropout=0.)\n",
    "rcx = rc(x)\n",
    "sa = SelfAttention(1, in_channel_divider=1)\n",
    "sa.query.register_forward_hook(hook_fn)\n",
    "sa.value.register_forward_hook(hook_fn)\n",
    "sa.key.register_forward_hook(hook_fn)\n",
    "sa.register_forward_hook(atn_hook_fn)\n",
    "output = sa(rcx)\n",
    "show_images(output)\n",
    "# TODO: register to view the output of attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, time_embedding_dim=10):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "\n",
    "        # Learnable embeddings\n",
    "        self.time_embedding = nn.Embedding(1000, time_embedding_dim)  # Assuming 1000 different timesteps\n",
    "        self.class_embedding = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = ResidualDoubleConv(in_channels + time_embedding_dim + num_classes, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.attn1 = SelfAttention(64)\n",
    "        self.down2 = ResidualDoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.attn2 = SelfAttention(128)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualDoubleConv(128, 256)\n",
    "        self.attn_bottleneck = SelfAttention(256)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv1 = ResidualDoubleConv(256, 128)\n",
    "        self.attn3 = SelfAttention(128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv2 = ResidualDoubleConv(128, 64)\n",
    "        self.attn4 = SelfAttention(64)\n",
    "\n",
    "        # Final Convolution\n",
    "        self.final_conv = nn.Conv2d(64, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embedding(t).view(t.size(0), self.time_embedding_dim, 1, 1)\n",
    "        t_emb = t_emb.expand(-1, -1, x.size(2), x.size(3))\n",
    "\n",
    "        # Class embedding\n",
    "        y_emb = self.class_embedding(y).view(y.size(0), self.num_classes, 1, 1)\n",
    "        y_emb = y_emb.expand(-1, -1, x.size(2), x.size(3))\n",
    "\n",
    "        # Concatenate input, time embedding, and class embedding\n",
    "        x = torch.cat([x, t_emb, y_emb], dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x1 = self.attn1(x1)\n",
    "        x = self.pool1(x1)\n",
    "        x2 = self.down2(x)\n",
    "        x2 = self.attn2(x2)\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.attn_bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.attn3(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attn4(x)\n",
    "\n",
    "        # Final Convolution\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = UNet(in_channels=1, num_classes=10).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "t = torch.zeros(x.shape[0]).long().to(device)\n",
    "y = y.long().to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "ic(f\"Shape of x: {x.shape}, t: {t.shape}, y: {y.shape}\")\n",
    "\n",
    "# Forward pass through the model\n",
    "ux = model(x, t, y)\n",
    "ic(ux.shape, x.shape)\n",
    "ic(torchvision.utils.make_grid(x.cpu())[0].shape)\n",
    "plt.imshow(torchvision.utils.make_grid(x.cpu())[0], cmap=\"Greys\")\n",
    "plt.show()\n",
    "plt.imshow(torchvision.utils.make_grid(ux.cpu())[0], cmap=\"Greys\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"UNet\"\n",
    "use_tiny_dataset = False\n",
    "n_epochs = 1\n",
    "\n",
    "last_epoch_nb = 0\n",
    "model_version = \"0.1\"\n",
    "if True:\n",
    "    train_model(\n",
    "        get_new_model=lambda: UNet(in_channels=1, num_classes=10).to(device),\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        n_epoch=5,\n",
    "        last_epoch=last_epoch_nb,\n",
    "        num_time_steps=num_time_steps,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        model_id=model_id,\n",
    "        batch_size=256\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_model_name(model_id, use_tiny_dataset=use_tiny_dataset)\n",
    "model = load_model(\n",
    "    model_name=model_name, model_version=model_version, iter=2\n",
    ").to(device)\n",
    "\n",
    "generate(model, numbers_to_gen=[0,1,2,3,4,5,6,7,8,9], scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-diffusion-04jlPuwc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
